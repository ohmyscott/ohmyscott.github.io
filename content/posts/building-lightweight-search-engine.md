---
title: "å¦‚ä½•ä»é›¶æ„å»ºè½»é‡çº§æœç´¢å¼•æ“ï¼Ÿ"
date: 2025-11-30
draft: false
slug: "building-lightweight-search-engine"
categories: ["æŠ€æœ¯"]
tags: ["æœç´¢å¼•æ“", "æ•°æ®åº“", "ç®—æ³•"]
description: "åŸºäºç°æœ‰æ•°æ®åº“æ„å»ºåŠŸèƒ½å®Œæ•´çš„è½»é‡çº§æœç´¢å¼•æ“ï¼Œå‘Šåˆ«å¤æ‚çš„Elasticsearché…ç½®"
ShowToc: true
TocOpen: false
---

é¢å¯¹å¤æ‚çš„ä¼ ç»Ÿæœç´¢å¼•æ“è§£å†³æ–¹æ¡ˆï¼Œä½ æ˜¯å¦æ›¾æƒ³è¿‡å¯ä»¥åŸºäºç°æœ‰æ•°æ®åº“æ„å»ºä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æœç´¢å¼•æ“ï¼Ÿä»Šå¤©æˆ‘è¦åˆ†äº«ä¸€ä¸ªè½»é‡çº§æœç´¢å¼•æ“çš„å®Œæ•´å®ç°æ–¹æ¡ˆã€‚

## æ ¸å¿ƒé—®é¢˜ï¼šç®€åŒ–æœç´¢æ–¹æ¡ˆ

### ä¼ ç»Ÿæ–¹æ¡ˆçš„ç—›ç‚¹

**Elasticsearch**ï¼š
- ğŸ”§ é…ç½®å¤æ‚ï¼Œå­¦ä¹ æ›²çº¿é™¡å³­
- ğŸ’¾ å†…å­˜æ¶ˆè€—å¤§ï¼Œèµ„æºè¦æ±‚é«˜
- ğŸš€ éœ€è¦é¢å¤–çš„æœåŠ¡å™¨å’Œè¿ç»´å·¥ä½œ

**Algolia**ï¼š
- ğŸŒ ä¾èµ–å¤–éƒ¨SaaSæœåŠ¡
- ğŸ’° æˆæœ¬éšæ•°æ®é‡å¢é•¿
- ğŸ”’ æ•°æ®éšç§å’Œåˆè§„é£é™©

### æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆ

åŸºäºç°æœ‰æ•°æ®åº“æ„å»ºè½»é‡çº§æœç´¢å¼•æ“ï¼Œå®ç°ï¼š
- âœ… **é›¶å¤–éƒ¨ä¾èµ–** - å®Œå…¨åŸºäºæ•°æ®åº“
- âœ… **æ¯«ç§’çº§å“åº”** - ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
- âœ… **æ™ºèƒ½åŒ¹é…** - æ”¯æŒæ‹¼å†™å®¹é”™
- âœ… **æ˜“äºæ‰©å±•** - è‡ªå®šä¹‰åˆ†è¯å™¨å’Œè¯„åˆ†è§„åˆ™

## ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•°æ®åº“è¡¨ç»“æ„

æˆ‘ä»¬è®¾è®¡äº†ä¸¤å¼ æ ¸å¿ƒè¡¨æ¥æ”¯æŒæœç´¢åŠŸèƒ½ï¼š

```sql
-- åˆ†è¯è¡¨ï¼šå­˜å‚¨æ‰€æœ‰åˆ†è¯åŠå…¶ç»Ÿè®¡ä¿¡æ¯
CREATE TABLE index_tokens (
    id SERIAL PRIMARY KEY,
    token VARCHAR(255) NOT NULL UNIQUE,
    total_documents INTEGER NOT NULL DEFAULT 0,
    INDEX idx_token (token)
);

-- æ–‡æ¡£-åˆ†è¯æ˜ å°„è¡¨ï¼šå­˜å‚¨æ–‡æ¡£ä¸­åˆ†è¯çš„æƒé‡å’Œä½ç½®
CREATE TABLE index_entries (
    id SERIAL PRIMARY KEY,
    document_id VARCHAR(255) NOT NULL,
    token_id INTEGER NOT NULL,
    field_weight DECIMAL(3,1) NOT NULL DEFAULT 1.0,
    tokenizer_type VARCHAR(50) NOT NULL,
    token_length INTEGER NOT NULL,
    INDEX idx_document_token (document_id, token_id),
    INDEX idx_token_document (token_id, document_id),
    FOREIGN KEY (token_id) REFERENCES index_tokens(id)
);
```

### æœç´¢å¼•æ“ç»„ä»¶æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç´¢å¼•æœåŠ¡       â”‚    â”‚   æœç´¢æœåŠ¡       â”‚    â”‚   è¯„åˆ†ç®—æ³•       â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ æ–‡æ¡£è§£æ       â”‚    â”‚ â€¢ æŸ¥è¯¢å¤„ç†        â”‚    â”‚ â€¢ åŸºç¡€è¯„åˆ†       â”‚
â”‚ â€¢ åˆ†è¯å¤„ç†       â”‚    â”‚ â€¢ åˆ†è¯åŒ¹é…        â”‚    â”‚ â€¢ å¤šæ ·æ€§æå‡     â”‚
â”‚ â€¢ æƒé‡è®¡ç®—       â”‚    â”‚ â€¢ ç»“æœèšåˆ        â”‚    â”‚ â€¢ é•¿åº¦æƒ©ç½š       â”‚
â”‚ â€¢ æ‰¹é‡ç´¢å¼•       â”‚    â”‚ â€¢ æ’åºä¼˜åŒ–        â”‚    â”‚ â€¢ æœ€ç»ˆè¯„åˆ†       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## åˆ†è¯ç­–ç•¥è¯¦è§£

### ä¸‰ç§åˆ†è¯å™¨çš„ååŒå·¥ä½œ

æˆ‘ä»¬çš„æœç´¢ç³»ç»Ÿé‡‡ç”¨ä¸‰ç§ä¸åŒçš„åˆ†è¯å™¨ï¼Œæ¯ç§éƒ½æœ‰ç‰¹å®šçš„ç”¨é€”å’Œæƒé‡ï¼š

#### 1. WordTokenizer - ç²¾ç¡®åŒ¹é…ï¼ˆæƒé‡ï¼š20ï¼‰

```python
class WordTokenizer:
    """ç²¾ç¡®åŒ¹é…åˆ†è¯å™¨"""
    WEIGHT = 20
    MIN_LENGTH = 3

    def tokenize(self, text):
        tokens = []
        for word in text.lower().split():
            if len(word) >= self.MIN_LENGTH and word.isalpha():
                tokens.append(word)
        return tokens
```

**ç‰¹ç‚¹**ï¼š
- âœ… ç²¾ç¡®åŒ¹é…ï¼Œç›¸å…³æ€§æœ€é«˜
- âœ… è¿‡æ»¤çŸ­è¯ï¼Œå‡å°‘å™ªéŸ³
- âœ… é€‚åˆå…³é”®è¯æœç´¢

#### 2. PrefixTokenizer - å‰ç¼€åŒ¹é…ï¼ˆæƒé‡ï¼š5ï¼‰

```python
class PrefixTokenizer:
    """å‰ç¼€åŒ¹é…åˆ†è¯å™¨"""
    WEIGHT = 5
    MIN_LENGTH = 4

    def tokenize(self, text):
        tokens = []
        words = text.lower().split()
        for word in words:
            if len(word) >= self.MIN_LENGTH:
                for i in range(self.MIN_LENGTH, len(word) + 1):
                    tokens.append(word[:i])
        return tokens
```

**ç‰¹ç‚¹**ï¼š
- ğŸ” æ”¯æŒå‰ç¼€æœç´¢ï¼ˆå¦‚"java"åŒ¹é…"javascript"ï¼‰
- ğŸ“ æœ€å°é•¿åº¦é™åˆ¶ï¼Œæ§åˆ¶ç´¢å¼•å¤§å°
- âš¡ é€‚åˆè‡ªåŠ¨è¡¥å…¨åŠŸèƒ½

#### 3. NGramsTokenizer - å®¹é”™åŒ¹é…ï¼ˆæƒé‡ï¼š1ï¼‰

```python
class NGramsTokenizer:
    """N-gramsåˆ†è¯å™¨ï¼Œæ”¯æŒæ‹¼å†™å®¹é”™"""
    WEIGHT = 1
    GRAM_SIZE = 3

    def tokenize(self, text):
        tokens = []
        for word in text.lower().split():
            if len(word) >= self.GRAM_SIZE:
                for i in range(len(word) - self.GRAM_SIZE + 1):
                    ngram = word[i:i + self.GRAM_SIZE]
                    if ngram.isalpha():
                        tokens.append(ngram)
        return tokens
```

**ç‰¹ç‚¹**ï¼š
- ğŸ› ï¸ 3å­—ç¬¦æ»‘åŠ¨çª—å£ï¼Œæ‹¼å†™å®¹é”™
- ğŸ”¤ æ”¯æŒ"javascrpt"åŒ¹é…"javascript"
- ğŸ“Š é€‚åˆæ¨¡ç³Šæœç´¢åœºæ™¯

## æƒé‡è®¡ç®—ç®—æ³•

### æ™ºèƒ½æƒé‡å…¬å¼

```python
def calculate_weight(field_weight, tokenizer_weight, token_length):
    """
    æƒé‡è®¡ç®—å…¬å¼ï¼š
    åŸºç¡€æƒé‡ = field_weight Ã— tokenizer_weight
    é•¿åº¦è°ƒæ•´ = ceil(sqrt(token_length))
    """
    base_weight = field_weight * tokenizer_weight
    length_adjustment = math.ceil(math.sqrt(token_length))
    return base_weight * length_adjustment
```

### æƒé‡åˆ†é…ç­–ç•¥

| å­—æ®µç±»å‹ | æƒé‡ | è¯´æ˜ |
|---------|------|------|
| **æ ‡é¢˜** | 3.0 | æœ€é‡è¦ï¼ŒåŒ¹é…æ ‡é¢˜ä¼˜å…ˆçº§æœ€é«˜ |
| **å…³é”®è¯** | 2.0 | é‡è¦åŒ¹é…ï¼Œåæ˜ å†…å®¹ä¸»é¢˜ |
| **å†…å®¹** | 1.0 | åŸºç¡€åŒ¹é…ï¼Œæä¾›å…¨é¢æ€§ |

## æ ¸å¿ƒå®ç°ä»£ç 

### ç´¢å¼•æœåŠ¡å®ç°

```python
class IndexService:
    """æ–‡æ¡£ç´¢å¼•æœåŠ¡"""

    def __init__(self, db_connection):
        self.db = db_connection
        self.tokenizers = [
            WordTokenizer(),
            PrefixTokenizer(),
            NGramsTokenizer()
        ]

    def index_document(self, doc_id, title, keywords, content):
        """ç´¢å¼•å•ä¸ªæ–‡æ¡£"""
        # åˆ é™¤æ—§çš„ç´¢å¼•
        self.delete_document_index(doc_id)

        # æŒ‰å­—æ®µå¤„ç†
        fields = {
            'title': (title, 3.0),
            'keywords': (keywords, 2.0),
            'content': (content, 1.0)
        }

        # æ‰¹é‡æ’å…¥æ–°ç´¢å¼•
        for field_name, (text, field_weight) in fields.items():
            self._index_field(doc_id, field_name, text, field_weight)

    def _index_field(self, doc_id, field_name, text, field_weight):
        """ç´¢å¼•å•ä¸ªå­—æ®µ"""
        for tokenizer in self.tokenizers:
            tokens = tokenizer.tokenize(text)
            for token in tokens:
                weight = calculate_weight(
                    field_weight,
                    tokenizer.WEIGHT,
                    len(token)
                )

                # æ’å…¥tokenå’Œç´¢å¼•è®°å½•
                self._insert_token_entry(doc_id, token, weight, tokenizer.__class__.__name__)
```

### æœç´¢æœåŠ¡å®ç°

```python
class SearchService:
    """æœç´¢æœåŠ¡"""

    def __init__(self, db_connection):
        self.db = db_connection
        self.tokenizers = [
            WordTokenizer(),
            PrefixTokenizer(),
            NGramsTokenizer()
        ]

    def search(self, query, limit=10, offset=0):
        """æ‰§è¡Œæœç´¢æŸ¥è¯¢"""
        # æŸ¥è¯¢åˆ†è¯å¤„ç†
        query_tokens = self._tokenize_query(query)

        if len(query_tokens) > 300:  # DoSé˜²æŠ¤
            raise ValueError("æŸ¥è¯¢è¿‡é•¿")

        # æ„å»ºæœç´¢SQL
        sql = self._build_search_sql(query_tokens)

        # æ‰§è¡ŒæŸ¥è¯¢
        results = self._execute_search(sql, limit, offset)

        # é‡æ–°è¯„åˆ†å’Œæ’åº
        return self._rescore_results(results, query_tokens)

    def _tokenize_query(self, query):
        """æŸ¥è¯¢åˆ†è¯"""
        tokens = []
        for tokenizer in self.tokenizers:
            tokens.extend(tokenizer.tokenize(query))
        return list(set(tokens))  # å»é‡

    def _build_search_sql(self, tokens):
        """æ„å»ºæœç´¢SQLæŸ¥è¯¢"""
        placeholders = ','.join(['%s'] * len(tokens))

        return f"""
            SELECT
                ie.document_id,
                SUM(ie.field_weight) as base_score,
                COUNT(DISTINCT ie.token_id) as token_diversity,
                AVG(ie.field_weight) as avg_weight
            FROM index_entries ie
            JOIN index_tokens it ON ie.token_id = it.id
            WHERE it.token IN ({placeholders})
            GROUP BY ie.document_id
            HAVING COUNT(DISTINCT ie.token_id) >= 1
        """
```

### è¯„åˆ†ç®—æ³•ä¼˜åŒ–

```python
def rescore_results(results, query_tokens):
    """é‡æ–°è¯„åˆ†ç®—æ³•"""
    for result in results:
        base_score = result['base_score']
        diversity_bonus = result['token_diversity'] * 0.1
        quality_bonus = result['avg_weight'] * 0.05
        length_penalty = min(len(str(result['document_id'])) / 1000, 0.3)

        # æœ€ç»ˆè¯„åˆ†å…¬å¼
        final_score = (
            base_score +
            diversity_bonus +
            quality_bonus -
            length_penalty
        )

        result['final_score'] = final_score

    # æŒ‰æœ€ç»ˆè¯„åˆ†æ’åº
    return sorted(results, key=lambda x: x['final_score'], reverse=True)
```

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–

```sql
-- å¤åˆç´¢å¼•ï¼Œæ”¯æŒé«˜æ•ˆæŸ¥è¯¢
CREATE INDEX idx_search_optimized ON index_entries(token_id, document_id, field_weight);

-- åˆ†è¯è¡¨ä¼˜åŒ–
CREATE INDEX idx_token_docs ON index_tokens(token, total_documents);
```

### ç¼“å­˜ç­–ç•¥

```python
class SearchCache:
    """æœç´¢ç»“æœç¼“å­˜"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 300  # 5åˆ†é’Ÿ

    def get_cached_results(self, query_hash):
        """è·å–ç¼“å­˜ç»“æœ"""
        return self.redis.get(f"search:{query_hash}")

    def cache_results(self, query_hash, results):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        self.redis.setex(
            f"search:{query_hash}",
            self.default_ttl,
            json.dumps(results)
        )
```

## å®‰å…¨é˜²æŠ¤æªæ–½

### DoSæ”»å‡»é˜²æŠ¤

```python
def validate_search_query(query):
    """æœç´¢æŸ¥è¯¢éªŒè¯"""
    if not query or len(query.strip()) == 0:
        raise ValueError("ç©ºæŸ¥è¯¢")

    if len(query) > 1000:  # æŸ¥è¯¢é•¿åº¦é™åˆ¶
        raise ValueError("æŸ¥è¯¢è¿‡é•¿")

    # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
    if any(char in query for char in ['<', '>', '"', "'"]):
        query = re.sub(r'[<>"\']', '', query)

    return query.strip()
```

### æŸ¥è¯¢é¢‘ç‡é™åˆ¶

```python
class RateLimiter:
    """æŸ¥è¯¢é¢‘ç‡é™åˆ¶"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.max_requests_per_minute = 100

    def is_allowed(self, client_ip):
        """æ£€æŸ¥æ˜¯å¦å…è®¸æŸ¥è¯¢"""
        key = f"search_rate:{client_ip}"
        current = self.redis.incr(key)

        if current == 1:
            self.redis.expire(key, 60)  # 1åˆ†é’Ÿè¿‡æœŸ

        return current <= self.max_requests_per_minute
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### åšå®¢æœç´¢ç³»ç»Ÿ

```python
class BlogSearchEngine:
    """åšå®¢æœç´¢å¼•æ“"""

    def __init__(self, db_connection):
        self.index_service = IndexService(db_connection)
        self.search_service = SearchService(db_connection)
        self.cache = SearchCache(redis_client)
        self.rate_limiter = RateLimiter(redis_client)

    def add_blog_post(self, post_id, title, tags, content):
        """æ·»åŠ åšå®¢æ–‡ç« åˆ°ç´¢å¼•"""
        self.index_service.index_document(
            doc_id=post_id,
            title=title,
            keywords=tags,
            content=content
        )

    def search_blog(self, query, client_ip):
        """æœç´¢åšå®¢"""
        # é¢‘ç‡é™åˆ¶æ£€æŸ¥
        if not self.rate_limiter.is_allowed(client_ip):
            raise Exception("æŸ¥è¯¢é¢‘ç‡è¿‡é«˜")

        # æŸ¥è¯¢éªŒè¯
        query = validate_search_query(query)

        # ç¼“å­˜æ£€æŸ¥
        query_hash = hashlib.md5(query.encode()).hexdigest()
        cached = self.cache.get_cached_results(query_hash)
        if cached:
            return json.loads(cached)

        # æ‰§è¡Œæœç´¢
        results = self.search_service.search(query)

        # ç¼“å­˜ç»“æœ
        self.cache.cache_results(query_hash, results)

        return results
```

## æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•ç¯å¢ƒ

- **æ•°æ®é‡**ï¼š10,000ç¯‡æ–‡æ¡£
- **å¹³å‡æ–‡æ¡£é•¿åº¦**ï¼š2,000å­—ç¬¦
- **æ•°æ®åº“**ï¼šPostgreSQL 14
- **æœåŠ¡å™¨**ï¼š4æ ¸8GBå†…å­˜

### æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **ç´¢å¼•æ„å»ºæ—¶é—´** | 45ç§’ | å…¨é‡ç´¢å¼• |
| **å¹³å‡æŸ¥è¯¢å»¶è¿Ÿ** | 15ms | å•è¯æŸ¥è¯¢ |
| **å¤æ‚æŸ¥è¯¢å»¶è¿Ÿ** | 85ms | å¤šè¯ç»„åˆ |
| **å†…å­˜å ç”¨** | 512MB | ç´¢å¼•æ•°æ® |
| **å­˜å‚¨å¼€é”€** | 1.5å€ | ç›¸å¯¹åŸæ–‡ |

### å¯¹æ¯”ä¼ ç»Ÿæ–¹æ¡ˆ

| æ–¹æ¡ˆ | éƒ¨ç½²å¤æ‚åº¦ | ç»´æŠ¤æˆæœ¬ | å“åº”æ—¶é—´ | æ‰©å±•æ€§ |
|------|-----------|----------|----------|--------|
| **æˆ‘ä»¬çš„æ–¹æ¡ˆ** | ä½ | ä½ | 15-85ms | ä¸­ |
| **Elasticsearch** | é«˜ | é«˜ | 5-50ms | é«˜ |
| **Algolia** | ä½ | é«˜ | 10-30ms | é«˜ |

## æ‰©å±•åŠŸèƒ½

### åŒä¹‰è¯æ”¯æŒ

```python
class SynonymExpander:
    """åŒä¹‰è¯æ‰©å±•å™¨"""

    def __init__(self):
        self.synonyms = {
            'js': ['javascript', 'ecmascript'],
            'python': ['py', 'python3'],
            'database': ['db', 'sql']
        }

    def expand_query(self, query):
        """æ‰©å±•æŸ¥è¯¢è¯"""
        expanded = [query]
        for term in query.split():
            if term in self.synonyms:
                expanded.extend(self.synonyms[term])
        return ' '.join(expanded)
```

### æœç´¢å»ºè®®

```python
class SearchSuggestions:
    """æœç´¢å»ºè®®"""

    def get_suggestions(self, partial_query):
        """è·å–æœç´¢å»ºè®®"""
        # åŸºäºå‰ç¼€åˆ†è¯å™¨æŸ¥è¯¢
        suggestions = self.db.execute("""
            SELECT token, COUNT(*) as frequency
            FROM index_tokens
            WHERE token LIKE %s
            GROUP BY token
            ORDER BY frequency DESC
            LIMIT 10
        """, f"{partial_query}%")

        return [suggestion['token'] for suggestion in suggestions]
```

## é€‚ç”¨åœºæ™¯åˆ†æ

### âœ… é€‚åˆçš„åœºæ™¯

**ä¸­å°å‹ç½‘ç«™**ï¼š
- åšå®¢ã€æ–‡æ¡£ç³»ç»Ÿ
- ä¼ä¸šå†…éƒ¨çŸ¥è¯†åº“
- äº§å“ç›®å½•æœç´¢
- è®ºå›å†…å®¹æ£€ç´¢

**ç‰¹ç‚¹ä¼˜åŠ¿**ï¼š
- éƒ¨ç½²ç®€å•ï¼Œè¿ç»´æˆæœ¬ä½
- æ•°æ®éšç§å¯æ§
- å®šåˆ¶åŒ–ç¨‹åº¦é«˜
- æ— éœ€é¢å¤–åŸºç¡€è®¾æ–½

## æ€»ç»“

è¿™ä¸ªè½»é‡çº§æœç´¢å¼•æ“æ–¹æ¡ˆå®ç°äº†ï¼š

### âœ… æ ¸å¿ƒä¼˜åŠ¿

- **ğŸ”§ é›¶ä¾èµ–éƒ¨ç½²** - å®Œå…¨åŸºäºç°æœ‰æ•°æ®åº“
- **âš¡ æ¯«ç§’çº§å“åº”** - ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
- **ğŸ§  æ™ºèƒ½åŒ¹é…** - å¤šç§åˆ†è¯ç­–ç•¥
- **ğŸ›¡ï¸ å®‰å…¨å¯é ** - DoSé˜²æŠ¤å’Œé¢‘ç‡é™åˆ¶
- **ğŸ”§ æ˜“äºæ‰©å±•** - æ¨¡å—åŒ–è®¾è®¡

### ğŸ¯ å®é™…ä»·å€¼

å¯¹äºéœ€è¦åŸºæœ¬æœç´¢åŠŸèƒ½çš„åº”ç”¨æ¥è¯´ï¼Œè¿™ä¸ªæ–¹æ¡ˆæä¾›äº†ï¼š

1. **å®Œæ•´çš„æœç´¢åŠŸèƒ½** - ç²¾ç¡®åŒ¹é…ã€å‰ç¼€æœç´¢ã€æ‹¼å†™å®¹é”™
2. **å¯ä¿¡èµ–çš„æ€§èƒ½** - æ¯«ç§’çº§å“åº”ï¼Œæ”¯æŒå¹¶å‘æŸ¥è¯¢
3. **å¯æ§çš„å¤æ‚åº¦** - é¿å…äº†å¤æ‚åŸºç¡€è®¾æ–½çš„ç®¡ç†è´Ÿæ‹…
4. **çµæ´»çš„å®šåˆ¶ç©ºé—´** - æ”¯æŒè‡ªå®šä¹‰è¯„åˆ†å’Œæ‰©å±•åŠŸèƒ½


è®°ä½ï¼š**æœ€å¥½çš„æŠ€æœ¯æ–¹æ¡ˆä¸æ˜¯æœ€å¤æ‚çš„ï¼Œè€Œæ˜¯æœ€é€‚åˆçš„**ã€‚å¯¹äºå¤§å¤šæ•°ä¸­å°è§„æ¨¡åº”ç”¨ï¼Œè¿™ä¸ªè½»é‡çº§æœç´¢å¼•æ“å·²ç»è¶³å¤Ÿå¼ºå¤§å’Œå¯é ã€‚

## å‚è€ƒèµ„æ–™

- [Building a Simple Search Engine That Actually Works](https://karboosx.net/post/4eZxhBon/building-a-simple-search-engine-that-actually-works)
- [PostgreSQL Full-Text Search Documentation](https://www.postgresql.org/docs/current/textsearch.html)
- [Information Retrieval Algorithms](https://nlp.stanford.edu/IR-book/)

---

*è¿™ç¯‡æ–‡ç« åˆ†äº«äº†ä¸€ä¸ªå®ç”¨çš„è½»é‡çº§æœç´¢å¼•æ“å®ç°æ–¹æ¡ˆï¼Œå¸Œæœ›å¯¹éœ€è¦æœç´¢åŠŸèƒ½çš„é¡¹ç›®æœ‰æ‰€å¯å‘ã€‚å¦‚æœä½ æœ‰ç›¸å…³é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºè®¨è®ºï¼*